{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a578525c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a MinIO en: http://localhost:9000\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "s3_endpoint = os.getenv(\"S3_API_ENDPOINT\", \"localhost:9000\")\n",
    "\n",
    "# Si el endpoint contiene \"minio:\" (nombre de servicio Docker), reemplazarlo por localhost\n",
    "# para que funcione cuando se ejecuta fuera de Docker\n",
    "if s3_endpoint.startswith(\"minio:\"):\n",
    "    s3_endpoint = s3_endpoint.replace(\"minio:\", \"localhost:\")\n",
    "\n",
    "minio_url = \"http://\" + s3_endpoint\n",
    "print(f\"Conectando a MinIO en: {minio_url}\")\n",
    "\n",
    "minio_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url=minio_url\n",
    ")\n",
    "\n",
    "minio_bucket = \"training-preparation-zone\"\n",
    "manifest_name = \"dataset_train.json\"\n",
    "local_file = \"./dataset_train.json\"\n",
    "\n",
    "manifest_name_augmented = \"dataset_train_augmented.json\"\n",
    "local_file_augmented = \"./dataset_train_augmented.json\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "023c4e2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./dataset_train.json\n",
      "./dataset_train_augmented.json\n"
     ]
    }
   ],
   "source": [
    "def download_manifest_from_minio(bucket_name, object_name, local_path):\n",
    "    try:\n",
    "        minio_client.download_file(bucket_name, object_name, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {object_name} from bucket {bucket_name}: {e}\")\n",
    "    return local_path\n",
    "\n",
    "downloaded_path = download_manifest_from_minio(minio_bucket, manifest_name, local_file)\n",
    "downloaded_path_augmented = download_manifest_from_minio(minio_bucket, manifest_name_augmented, local_file_augmented)\n",
    "\n",
    "print(downloaded_path)\n",
    "print(downloaded_path_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "388965ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 664 entries from the manifest.\n",
      "                                                 image  \\\n",
      "0    images/ISIC_0025899_rotated_-13_contrast_1.06.png   \n",
      "1    images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "2    images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "3    images/ISIC_0029577_brightness_0.87_contrast_0...   \n",
      "4                              images/ISIC_0031981.png   \n",
      "..                                                 ...   \n",
      "659  images/ISIC_0027219_rotated_10_brightness_1.03...   \n",
      "660                            images/ISIC_0031380.png   \n",
      "661                            images/ISIC_0025899.png   \n",
      "662  images/ISIC_0029694_brightness_1.06_contrast_0...   \n",
      "663  images/ISIC_0028103_rotated_2_contrast_0.89_fl...   \n",
      "\n",
      "                              text     score  \n",
      "0    texts/actinic_keratosis_0.txt  9934.281  \n",
      "1    texts/actinic_keratosis_1.txt  9933.505  \n",
      "2    texts/actinic_keratosis_2.txt  9932.875  \n",
      "3    texts/actinic_keratosis_3.txt  9935.860  \n",
      "4    texts/actinic_keratosis_4.txt  9969.258  \n",
      "..                             ...       ...  \n",
      "659       texts/melanoma_0_118.txt  9944.000  \n",
      "660       texts/melanoma_0_119.txt  9952.590  \n",
      "661       texts/melanoma_0_120.txt  9967.500  \n",
      "662       texts/melanoma_0_121.txt  9973.689  \n",
      "663       texts/melanoma_0_122.txt  9979.729  \n",
      "\n",
      "[664 rows x 3 columns]\n",
      "Loaded 1992 entries from the manifest.\n",
      "                                                  image  \\\n",
      "0                               images/ISIC_0025899.png   \n",
      "1       images/ISIC_0025899_brightness_1.12_flipped.png   \n",
      "2     images/ISIC_0025899_rotated_-13_contrast_1.06.png   \n",
      "3                               images/ISIC_0026803.png   \n",
      "4     images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "...                                                 ...   \n",
      "1987  images/ISIC_0029694_brightness_1.06_contrast_0...   \n",
      "1988      images/ISIC_0029694_contrast_1.15_flipped.png   \n",
      "1989                            images/ISIC_0028103.png   \n",
      "1990                  images/ISIC_0028103_augmented.png   \n",
      "1991  images/ISIC_0028103_rotated_2_contrast_0.89_fl...   \n",
      "\n",
      "                               text     score  \n",
      "0     texts/actinic_keratosis_0.txt  9934.281  \n",
      "1     texts/actinic_keratosis_0.txt  9934.281  \n",
      "2     texts/actinic_keratosis_0.txt  9934.281  \n",
      "3     texts/actinic_keratosis_1.txt  9933.505  \n",
      "4     texts/actinic_keratosis_1.txt  9933.505  \n",
      "...                             ...       ...  \n",
      "1987       texts/melanoma_0_121.txt  9973.689  \n",
      "1988       texts/melanoma_0_121.txt  9973.689  \n",
      "1989       texts/melanoma_0_122.txt  9979.729  \n",
      "1990       texts/melanoma_0_122.txt  9979.729  \n",
      "1991       texts/melanoma_0_122.txt  9979.729  \n",
      "\n",
      "[1992 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_manifest(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        data = pd.read_json(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from the manifest.\")\n",
    "    return data\n",
    "\n",
    "df = load_manifest(downloaded_path)\n",
    "print(df)\n",
    "\n",
    "df_augmented = load_manifest(downloaded_path_augmented)\n",
    "print(df_augmented)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67446b5",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7845cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1277e",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "\n",
    "The inputs variable is defined as it is because the model needs all of those parameters:\n",
    "\n",
    "- Truncation=True means that if we provide more than 77 tokens (the usual maximum) it truncates the data\n",
    "\n",
    "- Padding=max_length means that we add zeros to fill the max_length. We need to provide the same length for all the data (specially in text).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be17d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, minio_client, bucket_name):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.minio_client = minio_client\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.df.iloc[idx]['image']\n",
    "        txt_key = self.df.iloc[idx]['text']\n",
    "\n",
    "        img_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=img_key)\n",
    "        img_bytes = img_response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        txt_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=txt_key)\n",
    "        description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[description], \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430fa7e",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Here we train the smaller clip model. We load it from the SkinLesionDataset class we created and the particularity is that we use AdamW. The AdamW is a widely used optimitzer for training Transformers. While the loss function tells the model where it needs to go, the optimitzer decides how fast it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c0ca4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/huggingface_hub/file_download.py:801: UserWarning: Not enough free disk space to download the file. The expected file size is: 605.16 MB. The target location /Users/carlesaguilera/.cache/huggingface/hub/models--openai--clip-vit-base-patch32/blobs only has 151.35 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "dataset = SkinLesionDataset(df, processor, minio_client, minio_bucket)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162603b8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we train the model using the hyperparameters and all the information provided in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5f9fe3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/83 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NoSuchKey",
     "evalue": "An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoSuchKey\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m      9\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     10\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[24], line 17\u001b[0m, in \u001b[0;36mSkinLesionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m img_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     15\u001b[0m txt_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdf\u001b[38;5;241m.\u001b[39miloc[idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 17\u001b[0m img_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminio_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m img_bytes \u001b[38;5;241m=\u001b[39m img_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m     19\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(img_bytes))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    599\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    600\u001b[0m     )\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     error_code \u001b[38;5;241m=\u001b[39m request_context\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m   1075\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror_code_override\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1076\u001b[0m     ) \u001b[38;5;129;01mor\u001b[39;00m error_info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCode\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1080\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parsed_response\n",
      "\u001b[0;31mNoSuchKey\u001b[0m: An error occurred (NoSuchKey) when calling the GetObject operation: The specified key does not exist."
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            return_loss=True\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    loss_history.append(epoch_loss / len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'Recall@1': np.float64(0.10843373493975904), 'Recall@5': np.float64(0.3614457831325301), 'Recall@10': np.float64(0.5542168674698795), 'Mean Rank': np.float64(16.236144578313255), 'Median Rank': np.float64(9.0), 'MRR': np.float64(0.24258346426257882), 'NDCG': np.float64(0.396844550944322)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score_func\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def extract_class_from_path(path):\n",
    "    return \"_\".join(path.split(\"/\")[-1].split(\"_\")[:-3])\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_comprehensive_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    all_ground_truth_texts = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        img_emb = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        txt_emb = model.get_text_features(input_ids=batch['input_ids'], \n",
    "                                        attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        all_image_embeds.append(F.normalize(img_emb, dim=-1))\n",
    "        all_text_embeds.append(F.normalize(txt_emb, dim=-1))\n",
    "\n",
    "    image_embeds = torch.cat(all_image_embeds)\n",
    "    text_embeds = torch.cat(all_text_embeds)\n",
    "\n",
    "    # Perspective 1: Text-to-Image Retrieval\n",
    "    sim_matrix = text_embeds @ image_embeds.T\n",
    "    \n",
    "    num_queries = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "\n",
    "    # Perspective 2: Safety (Clinical Classification)\n",
    "    # We pass an image, retrieve the best text and check if classes match.\n",
    "    sim_matrix_i2t = image_embeds @ text_embeds.T\n",
    "    \n",
    "    # Map all text files in the dataset to their classes\n",
    "    all_text_paths = df['text'].tolist()\n",
    "    text_classes = np.array([extract_class_from_path(p) for p in all_text_paths])\n",
    "    image_classes = np.array([extract_class_from_path(p) for p in df['text'].tolist()])\n",
    "\n",
    "    top_text_indices = torch.argmax(sim_matrix_i2t, dim=-1).cpu().numpy()\n",
    "    predicted_classes = text_classes[top_text_indices]\n",
    "\n",
    "    sensitivity = recall_score(image_classes, predicted_classes, average='macro')\n",
    "    f1 = f1_score(image_classes, predicted_classes, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(image_classes, predicted_classes)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    specificity = np.mean(tn / (tn + fp + 1e-10))\n",
    "\n",
    "    def get_text_content(path, client, bucket):\n",
    "        response = client.get_object(Bucket=bucket, Key=path)\n",
    "        return response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "    sample_indices = np.random.choice(len(df), min(50, len(df)), replace=False)\n",
    "    gt_texts = [get_text_content(df.iloc[i]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "    predicted_classes_texts = [get_text_content(df.iloc[top_text_indices[i]]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "\n",
    "    _, _, bert_f1 = bert_score_func(predicted_classes_texts, gt_texts, lang='en', verbose=False)\n",
    "\n",
    "    metrics = {\n",
    "        # Perspective 1\n",
    "        \"Recall@1\":  np.mean(ranks <= 1),\n",
    "        \"Recall@5\":  np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"Mean Rank\": np.mean(ranks),\n",
    "        \"Median Rank\": np.median(ranks),\n",
    "        \"MRR\": np.mean(1.0 / ranks),\n",
    "        \"NDCG\": np.mean([1.0 / np.log2(r + 1) for r in ranks]),\n",
    "        # Perspective 2\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1 Score\": f1,\n",
    "        # Perspective 3\n",
    "        \"BERTScore F1\": bert_f1.mean().item()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "eval_results = get_comprehensive_metrics(model, dataloader, DEVICE)\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
