{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bfebed1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conectando a MinIO en: http://localhost:9000\n"
     ]
    }
   ],
   "source": [
    "# Configuración inicial: Conexión a MinIO\n",
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "s3_endpoint = os.getenv(\"S3_API_ENDPOINT\", \"localhost:9000\")\n",
    "\n",
    "# Si el endpoint contiene \"minio:\" (nombre de servicio Docker), reemplazarlo por localhost\n",
    "if s3_endpoint.startswith(\"minio:\"):\n",
    "    s3_endpoint = s3_endpoint.replace(\"minio:\", \"localhost:\")\n",
    "\n",
    "minio_url = \"http://\" + s3_endpoint\n",
    "print(f\"Conectando a MinIO en: {minio_url}\")\n",
    "\n",
    "minio_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url=minio_url\n",
    ")\n",
    "\n",
    "minio_bucket = \"training-preparation-zone\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e0d182ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset train: 664 entradas\n",
      "Dataset train augmented: 1992 entradas\n",
      "\n",
      "Primeras filas del dataset train:\n",
      "                                               image  \\\n",
      "0  images/ISIC_0025899_rotated_-13_contrast_1.06.png   \n",
      "1  images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "2  images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "3  images/ISIC_0029577_brightness_0.87_contrast_0...   \n",
      "4                            images/ISIC_0031981.png   \n",
      "\n",
      "                            text     score  \n",
      "0  texts/actinic_keratosis_0.txt  9934.281  \n",
      "1  texts/actinic_keratosis_1.txt  9933.505  \n",
      "2  texts/actinic_keratosis_2.txt  9932.875  \n",
      "3  texts/actinic_keratosis_3.txt  9935.860  \n",
      "4  texts/actinic_keratosis_4.txt  9969.258  \n"
     ]
    }
   ],
   "source": [
    "# Cargar datasets locales\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Cargar dataset_train.json\n",
    "with open(\"./dataset_train.json\", 'r') as f:\n",
    "    df_train = pd.read_json(f)\n",
    "\n",
    "# Cargar dataset_train_augmented.json\n",
    "with open(\"./dataset_train_augmented.json\", 'r') as f:\n",
    "    df_train_augmented = pd.read_json(f)\n",
    "\n",
    "print(f\"Dataset train: {len(df_train)} entradas\")\n",
    "print(f\"Dataset train augmented: {len(df_train_augmented)} entradas\")\n",
    "print(f\"\\nPrimeras filas del dataset train:\")\n",
    "print(df_train.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c6729c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dispositivo: cpu\n",
      "Modelo base: openai/clip-vit-base-patch32\n",
      "Modelo potente: openai/clip-vit-large-patch14\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# Hiperparámetros\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "MODEL_LARGE_ID = \"openai/clip-vit-large-patch14\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 3\n",
    "\n",
    "print(f\"Dispositivo: {DEVICE}\")\n",
    "print(f\"Modelo base: {MODEL_ID}\")\n",
    "print(f\"Modelo potente: {MODEL_LARGE_ID}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092357e6",
   "metadata": {},
   "source": [
    "## Dataset Class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee1ec061",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, minio_client, bucket_name):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.minio_client = minio_client\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.df.iloc[idx]['image']\n",
    "        txt_key = self.df.iloc[idx]['text']\n",
    "\n",
    "        # Intentar cargar imagen del bucket principal\n",
    "        try:\n",
    "            img_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=img_key)\n",
    "            img_bytes = img_response['Body'].read()\n",
    "        except Exception as e:\n",
    "            # Si no está en el bucket principal, buscar en otros buckets\n",
    "            img_bytes = None\n",
    "            for bucket in [\"augmentation-zone\", \"exploitation-zone\"]:\n",
    "                try:\n",
    "                    img_response = self.minio_client.get_object(Bucket=bucket, Key=img_key)\n",
    "                    img_bytes = img_response['Body'].read()\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if img_bytes is None:\n",
    "                raise FileNotFoundError(f\"Imagen no encontrada: {img_key}\")\n",
    "        \n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        # Intentar cargar texto del bucket principal\n",
    "        try:\n",
    "            txt_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=txt_key)\n",
    "            description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "        except Exception as e:\n",
    "            # Si no está en el bucket principal, buscar en otros buckets\n",
    "            description = None\n",
    "            for bucket in [\"augmentation-zone\", \"exploitation-zone\"]:\n",
    "                try:\n",
    "                    txt_response = self.minio_client.get_object(Bucket=bucket, Key=txt_key)\n",
    "                    description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "                    break\n",
    "                except Exception:\n",
    "                    continue\n",
    "            \n",
    "            if description is None:\n",
    "                raise FileNotFoundError(f\"Texto no encontrado: {txt_key}\")\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[description], \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f4a9e8",
   "metadata": {},
   "source": [
    "## TRAIN BASE MODEL CLIP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cbc1ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo CLIP base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo CLIP base cargado\n",
      "\n",
      "Creando dataset con 1992 entradas...\n",
      "Dataset creado: 1992 muestras\n",
      "Dataloader creado: 249 batches\n"
     ]
    }
   ],
   "source": [
    "# Inicializar modelo CLIP base\n",
    "print(\"Cargando modelo CLIP base...\")\n",
    "model_base = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor_base = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "print(\"Modelo CLIP base cargado\")\n",
    "\n",
    "# Crear dataset y dataloader con el dataset aumentado\n",
    "print(f\"\\nCreando dataset con {len(df_train_augmented)} entradas...\")\n",
    "dataset_train = SkinLesionDataset(df_train_augmented, processor_base, minio_client, minio_bucket)\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model_base.parameters(), lr=LEARNING_RATE, weight_decay=0.1)\n",
    "\n",
    "print(f\"Dataset creado: {len(dataset_train)} muestras\")\n",
    "print(f\"Dataloader creado: {len(dataloader_train)} batches\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59606b76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando entrenamiento...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   0%|          | 0/249 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Imagen no encontrada: images/ISIC_0025899_brightness_1.12_flipped.png",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mClientError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m, in \u001b[0;36mSkinLesionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     img_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mminio_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBucket\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m     img_bytes \u001b[38;5;241m=\u001b[39m img_response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBody\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/client.py:602\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    601\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_api_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43moperation_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/context.py:123\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m     hook()\n\u001b[0;32m--> 123\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/botocore/client.py:1078\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     error_class \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexceptions\u001b[38;5;241m.\u001b[39mfrom_code(error_code)\n\u001b[0;32m-> 1078\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m error_class(parsed_response, operation_name)\n\u001b[1;32m   1079\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[0;31mClientError\u001b[0m: An error occurred (RequestTimeTooSkewed) when calling the GetObject operation: The difference between the request time and the server's time is too large.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m pbar \u001b[38;5;241m=\u001b[39m tqdm(dataloader_train, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m pbar:\n\u001b[1;32m     11\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     12\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(DEVICE) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:732\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    730\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 732\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    734\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    735\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    736\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    738\u001b[0m ):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/dataloader.py:788\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    787\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 788\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    789\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    790\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mSkinLesionDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m img_bytes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImagen no encontrada: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     35\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(io\u001b[38;5;241m.\u001b[39mBytesIO(img_bytes))\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Intentar cargar texto del bucket principal\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Imagen no encontrada: images/ISIC_0025899_brightness_1.12_flipped.png"
     ]
    }
   ],
   "source": [
    "# Entrenamiento del modelo CLIP base\n",
    "loss_history = []\n",
    "model_base.train()\n",
    "\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(dataloader_train, desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model_base(\n",
    "            input_ids=batch['input_ids'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            return_loss=True\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader_train)\n",
    "    loss_history.append(avg_loss)\n",
    "    print(f\"Epoch {epoch+1} completada - Pérdida promedio: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"\\n Entrenamiento completado!\")\n",
    "print(f\"Pérdidas por época: {loss_history}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8263a3c",
   "metadata": {},
   "source": [
    "## Cargar Modelo CLIP Potente para Comparación\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edee2b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo CLIP potente (sin entrenar)\n",
    "print(\"Cargando modelo CLIP potente...\")\n",
    "model_large = CLIPModel.from_pretrained(MODEL_LARGE_ID).to(DEVICE)\n",
    "processor_large = CLIPProcessor.from_pretrained(MODEL_LARGE_ID)\n",
    "print(\"✓ Modelo CLIP potente cargado\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d13ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para evaluar modelos\n",
    "@torch.no_grad()\n",
    "def evaluate_model(model, processor, dataframe, minio_client, bucket_name, device, model_name):\n",
    "    \"\"\"Evalúa un modelo CLIP y retorna métricas\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    # Crear dataset y dataloader\n",
    "    dataset = SkinLesionDataset(dataframe, processor, minio_client, bucket_name)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    \n",
    "    print(f\"Evaluando {model_name}...\")\n",
    "    for batch in tqdm(dataloader, desc=f\"Procesando {model_name}\"):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        img_emb = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        txt_emb = model.get_text_features(\n",
    "            input_ids=batch['input_ids'], \n",
    "            attention_mask=batch['attention_mask']\n",
    "        )\n",
    "        \n",
    "        all_image_embeds.append(F.normalize(img_emb, dim=-1))\n",
    "        all_text_embeds.append(F.normalize(txt_emb, dim=-1))\n",
    "    \n",
    "    image_embeds = torch.cat(all_image_embeds)\n",
    "    text_embeds = torch.cat(all_text_embeds)\n",
    "    \n",
    "    # Text-to-Image Retrieval\n",
    "    sim_matrix = text_embeds @ image_embeds.T\n",
    "    num_queries = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
    "        rank_idx = torch.where(sorted_indices == i)[0]\n",
    "        if len(rank_idx) > 0:\n",
    "            rank = rank_idx[0].item() + 1\n",
    "        else:\n",
    "            rank = len(sorted_indices) + 1\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    metrics = {\n",
    "        \"Recall@1\": np.mean(ranks <= 1),\n",
    "        \"Recall@5\": np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"Mean Rank\": np.mean(ranks),\n",
    "        \"Median Rank\": np.median(ranks),\n",
    "        \"MRR\": np.mean(1.0 / ranks),\n",
    "        \"NDCG\": np.mean([1.0 / np.log2(r + 1) for r in ranks]),\n",
    "    }\n",
    "    \n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0dc84da",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cca5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo base entrenado\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUANDO MODELO BASE ENTRENADO\")\n",
    "print(\"=\" * 60)\n",
    "metrics_base = evaluate_model(\n",
    "    model_base, processor_base, df_train, \n",
    "    minio_client, minio_bucket, DEVICE, \"CLIP Base (Entrenado)\"\n",
    ")\n",
    "\n",
    "print(\"\\nMétricas del modelo base entrenado:\")\n",
    "for key, value in metrics_base.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar modelo potente (sin entrenar)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVALUANDO MODELO CLIP POTENTE (SIN ENTRENAR)\")\n",
    "print(\"=\" * 60)\n",
    "metrics_large = evaluate_model(\n",
    "    model_large, processor_large, df_train,\n",
    "    minio_client, minio_bucket, DEVICE, \"CLIP Large (Sin entrenar)\"\n",
    ")\n",
    "\n",
    "print(\"\\nMétricas del modelo CLIP potente:\")\n",
    "for key, value in metrics_large.items():\n",
    "    print(f\"  {key}: {value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa266e48",
   "metadata": {},
   "source": [
    "## Comparación de Resultados\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25126b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "COMPARACIÓN DE MODELOS\n",
      "============================================================\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCOMPARACIÓN DE MODELOS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m comparison_df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMétrica\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mlist\u001b[39m(metrics_base\u001b[38;5;241m.\u001b[39mkeys()),\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP Base (Entrenado)\u001b[39m\u001b[38;5;124m\"\u001b[39m: [metrics_base[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m metrics_base\u001b[38;5;241m.\u001b[39mkeys()],\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP Large (Sin entrenar)\u001b[39m\u001b[38;5;124m\"\u001b[39m: [metrics_large[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m metrics_large\u001b[38;5;241m.\u001b[39mkeys()],\n\u001b[1;32m     10\u001b[0m })\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Calcular diferencia\u001b[39;00m\n\u001b[1;32m     13\u001b[0m comparison_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDiferencia\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m comparison_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP Base (Entrenado)\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m-\u001b[39m comparison_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCLIP Large (Sin entrenar)\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Comparación de resultados\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"COMPARACIÓN DE MODELOS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    \"Métrica\": list(metrics_base.keys()),\n",
    "    \"CLIP Base (Entrenado)\": [metrics_base[k] for k in metrics_base.keys()],\n",
    "    \"CLIP Large (Sin entrenar)\": [metrics_large[k] for k in metrics_large.keys()],\n",
    "})\n",
    "\n",
    "# Calcular diferencia\n",
    "comparison_df[\"Diferencia\"] = comparison_df[\"CLIP Base (Entrenado)\"] - comparison_df[\"CLIP Large (Sin entrenar)\"]\n",
    "comparison_df[\"Mejor\"] = comparison_df.apply(\n",
    "    lambda row: \"Base\" if row[\"Diferencia\"] > 0 else \"Large\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "print(\"\\nTabla comparativa:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "# Resumen\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RESUMEN\")\n",
    "print(\"=\" * 60)\n",
    "base_wins = (comparison_df[\"Diferencia\"] > 0).sum()\n",
    "large_wins = (comparison_df[\"Diferencia\"] < 0).sum()\n",
    "\n",
    "print(f\"\\nMétricas donde CLIP Base (Entrenado) es mejor: {base_wins}\")\n",
    "print(f\"Métricas donde CLIP Large (Sin entrenar) es mejor: {large_wins}\")\n",
    "\n",
    "if base_wins > large_wins:\n",
    "    print(\"\\n✓ El modelo CLIP Base entrenado supera al modelo CLIP Large en la mayoría de métricas!\")\n",
    "elif large_wins > base_wins:\n",
    "    print(\"\\n⚠ El modelo CLIP Large sin entrenar supera al modelo base entrenado.\")\n",
    "else:\n",
    "    print(\"\\n≈ Los modelos tienen un rendimiento similar.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
