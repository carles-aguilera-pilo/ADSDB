{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9adf352d",
   "metadata": {},
   "source": [
    "# Obtaining the data from datasources\n",
    "\n",
    "This project focuses on the design and execution of a data processing pipeline that transforms raw data from multiple sources into a structued format suitable for machine learning applications.\n",
    "\n",
    "In a real scenario, such data sources would come from different repositories or systems, for example, different databases or APIs, each containing only a subset of the required information. However, these idealized and openly accessible data sources do not always exist in practice, especially when the project involves combining multiple types of information. To overcome this limitation and for academic purposes, we have created our own data sources by extracting, restructuring, and combining data from existing public datasets.\n",
    "\n",
    "This notebook therefore begins by introducing the different simulated data sources we created, explaining their purpose and structure. After that, in the [temporal landing zone](./temporal_zone.ipynb) we demonstrate how the pipeline ingests theses sources, and futher applies transformations to produce a final dataset ready for use in ML models.\n",
    "\n",
    "The different datasources we used or created are the following:\n",
    "1. [huggingface dataset](https://huggingface.co/datasets/Moaaz55/skin_cancer_questions_answers). This dataset is used for ...\n",
    "2. Self made audio dataset. This dataset is used for ...\n",
    "3. Wikipedia web scrapping information. This dataset is used for...\n",
    "\n",
    "In the next sections, we will discuss how the data is obtained to further insert them into the first zone of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da091db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import mkdir\n",
    "\n",
    "OUTPUT_DIR = \"../../output/\"\n",
    "datasets = 3\n",
    "\n",
    "for i in range(1, datasets + 1):\n",
    "    try:\n",
    "        mkdir(OUTPUT_DIR + f\"dataset{i}/\")\n",
    "    except FileExistsError:\n",
    "        pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a733d14",
   "metadata": {},
   "source": [
    "## HAM10000\n",
    "\n",
    "The dataset \"Ham10000\" stands for *Human Against Machine with 10000 training images*. It is a benchmark dataset of dermatoscopic images of pigmented skin lesions for research in skin lesion classification and related tasks.\n",
    "It contains over 10,015 dermatoscopic images of pigmented lesions and covers seven diagnostic categories of pigmented skin lesion types.\n",
    "\n",
    "Our dataset is the **abaryan/ham10000_bbox** variant. This is a enhanced version of the original HAM10000, with extra spatial annotations. This new annotations include bouding box coordinates for lesion localization, an area coverage giving the relative proportion of the bouding box relative to the entire image and some natural language descriptions to describe where the lesion lies in the image.\n",
    "**Change abaryan/ham10000_bbox dataset with a dataset not containing useless information**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c1191422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (7/7 shards): 100%|██████████| 8012/8012 [00:04<00:00, 1880.38 examples/s]\n",
      "Saving the dataset (2/2 shards): 100%|██████████| 2003/2003 [00:01<00:00, 1925.03 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"abaryan/ham10000_bbox\")\n",
    "ds.save_to_disk(OUTPUT_DIR + \"dataset_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914a55dc",
   "metadata": {},
   "source": [
    "## Wikipedia web scrapping\n",
    "\n",
    "In this section we want to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c2a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "topics = [\"skin_cancer\", \"melanoma\", \"basal_cell_carcinoma\", \"squamous_cell_carcinoma\", \"actinic_keratosis\"]\n",
    "#topics = [\"skin_cancer\"]\n",
    "\n",
    "def wikipedia_scrapper(topics):\n",
    "    for topic in topics:\n",
    "        url = 'https://en.wikipedia.org/wiki/' + topic\n",
    "        user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3\"\n",
    "        res = requests.get(url, headers={'User-Agent': user_agent})\n",
    "        soup = BeautifulSoup(res.text, 'html.parser')\n",
    "\n",
    "        data = ''\n",
    "        for p in soup.find_all('p'):\n",
    "            data += p.get_text()\n",
    "            data += '\\n'\n",
    "        \n",
    "        data = data.strip()\n",
    "\n",
    "        for j in range(1, 750):\n",
    "            data = data.replace('[' + str(j) + ']', '')\n",
    "\n",
    "\n",
    "        fd = open(OUTPUT_DIR + \"dataset2/\" + topic + '.txt', 'w')\n",
    "        fd.write(data)\n",
    "        fd.close()\n",
    "\n",
    "wikipedia_scrapper(topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ed3300",
   "metadata": {},
   "source": [
    "## Self made audio dataset\n",
    "\n",
    "Explicar self made audio dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6e45004",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriol/Documentos/MDS/ADSDB/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "df = pd.read_json(\"hf://datasets/Moaaz55/skin_cancer_questions_answers/dataset.json\", lines=True)\n",
    "\n",
    "def limpiar_dataset(df, columna='Answer'):\n",
    "    # Convertir a string y limpiar\n",
    "    df_temp = df.copy()\n",
    "    df_temp[columna] = df_temp[columna].astype(str)\n",
    "    \n",
    "    # filter valid answers.\n",
    "    df_limpio = df_temp[\n",
    "        df_temp[columna].notna() &\n",
    "        (df_temp[columna].str.strip() != '') &\n",
    "        (df_temp[columna].str.strip() != 'nan') &\n",
    "        (df_temp[columna].str.strip() != 'None') &\n",
    "        (df_temp[columna].str.strip() != 'null') &\n",
    "        (df_temp[columna].str.len() > 10)  # Mínimo 10 caracteres\n",
    "    ]\n",
    "    \n",
    "    print(f\" Limpieza completada:\")\n",
    "    print(f\"  Original: {len(df)} filas\")\n",
    "    print(f\"  Limpio: {len(df_limpio)} filas\")\n",
    "    print(f\"  Eliminadas: {len(df) - len(df_limpio)} filas\")\n",
    "    \n",
    "    return df_limpio\n",
    "\n",
    "\n",
    "#df = limpiar_dataset(df)\n",
    "df = df.sample(n=100, random_state=42)\n",
    "df_text = df\n",
    "df_text = df.apply(lambda row: f\"A: {row['Answer']}\\n\", axis=1)\n",
    "#df_text = df.apply(lambda row: f\"Q: {row['Question']}\\nA: {row['Answer']}\\n\", axis=1)\n",
    "df_text\n",
    "with open(OUTPUT_DIR + \"dataset3/dataset3.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"\\n\".join(df_text.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c9e61e",
   "metadata": {},
   "source": [
    "## Model instal·lation\n",
    "\n",
    "To install the model, you must execute this command:\n",
    "```sh\n",
    "python3 -m piper.download_voices en_US-lessac-medium\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c013d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wave\n",
    "from piper import PiperVoice\n",
    "import os\n",
    "\n",
    "voice = PiperVoice.load(os.getcwd() + \"/en_US-lessac-medium.onnx\")\n",
    "\n",
    "for i, text in enumerate(df_text):\n",
    "    text = text.replace(\"A: \", \"\").strip()\n",
    "    with wave.open(OUTPUT_DIR + f\"dataset3/answer_{i}.wav\", \"wb\") as wav_file:\n",
    "        voice.synthesize_wav(text, wav_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f535d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT THE TEXT TO AUDIO WITH THE TT'S LIBRARY\n",
    "import os\n",
    "import io\n",
    "import hashlib\n",
    "from datetime import datetime\n",
    "from gtts import gTTS\n",
    "from mutagen.mp3 import MP3\n",
    "\n",
    "# CONFIGRATION PARAMETERS\n",
    "OUT_DIR = \"output_audio\"\n",
    "LANG = \"es\"\n",
    "TEXT_COL = \"Answer\"\n",
    "\n",
    "# CREATE AUDIO AND METADATA DIRECTORIES\n",
    "def ensure_dirs(root):\n",
    "    audio_dir = os.path.join(root, \"audio\")\n",
    "    metadata_dir = os.path.join(root, \"metadata\")\n",
    "    os.makedirs(audio_dir, exist_ok=True)\n",
    "    os.makedirs(metadata_dir, exist_ok=True)\n",
    "    return audio_dir, metadata_dir\n",
    "\n",
    "# FUNCTION TO GENERATE MD5 HASH (AVOID DUPLICATES)\n",
    "def md5(s: str) -> str:\n",
    "    return hashlib.md5(s.encode(\"utf-8\")).hexdigest()\n",
    "\n",
    "# CONVERT TEXT TO AUDIO BYTES\n",
    "def tts_bytes(text: str, lang: str) -> bytes:\n",
    "    buf = io.BytesIO()\n",
    "    gTTS(text=text, lang=lang).write_to_fp(buf)\n",
    "    buf.seek(0)\n",
    "    return buf.read()\n",
    "\n",
    "# GET MP3 DURATION\n",
    "def mp3_duration(b: bytes):\n",
    "    try:\n",
    "        return float(MP3(io.BytesIO(b)).info.length)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# MAIN FUNCTION TO CONVERT ANSWERS TO AUDIO\n",
    "def answers_to_audio(df: pd.DataFrame):\n",
    "    audio_dir, meta_dir = ensure_dirs(OUT_DIR)\n",
    "    df_out = df.copy()\n",
    "\n",
    "    # Add columns for audio metadata\n",
    "    df_out[\"Answer_audio_relpath\"] = None\n",
    "    df_out[\"Answer_duration_sec\"] = None\n",
    "    df_out[\"Answer_size_bytes\"] = None\n",
    "    df_out[\"Answer_text_md5\"] = None\n",
    "\n",
    "    total = len(df_out)\n",
    "    for i, text in enumerate(df_out[TEXT_COL].astype(str)):\n",
    "        if not text.strip():\n",
    "            continue\n",
    "\n",
    "        h = md5(text)\n",
    "        filename = f\"answer_{h}.mp3\"\n",
    "        abspath = os.path.join(audio_dir, filename)\n",
    "        relpath = os.path.join(\"audio\", filename)\n",
    "\n",
    "        # Only generate if it doesn't exist\n",
    "        if not os.path.exists(abspath):\n",
    "            mp3 = tts_bytes(text, LANG)\n",
    "            with open(abspath, \"wb\") as f:\n",
    "                f.write(mp3)\n",
    "            size = len(mp3)\n",
    "            dur = mp3_duration(mp3)\n",
    "        else:\n",
    "            with open(abspath, \"rb\") as f:\n",
    "                data = f.read()\n",
    "            size = len(data)\n",
    "            dur = mp3_duration(data)\n",
    "\n",
    "        df_out.at[i, \"Answer_audio_relpath\"] = relpath.replace(\"\\\\\", \"/\")\n",
    "        df_out.at[i, \"Answer_duration_sec\"] = dur\n",
    "        df_out.at[i, \"Answer_size_bytes\"] = size\n",
    "        df_out.at[i, \"Answer_text_md5\"] = h\n",
    "\n",
    "        if (i+1) % 50 == 0 or i+1 == total:\n",
    "            print(f\"[{i+1}/{total}] {relpath} ({dur:.2f}s)\")\n",
    "\n",
    "    # Save enriched dataset\n",
    "    today = datetime.utcnow().strftime(\"%Y-%m-%d\")\n",
    "    out_latest = os.path.join(meta_dir, \"answers_dataset-latest.parquet\")\n",
    "    out_dated = os.path.join(meta_dir, f\"answers_dataset-{today}.parquet\")\n",
    "\n",
    "    df_out.to_parquet(out_latest, index=False)\n",
    "    df_out.to_parquet(out_dated, index=False)\n",
    "\n",
    "    print(\"\\n Listo\")\n",
    "    print(\"Audios →\", os.path.abspath(audio_dir))\n",
    "    print(\"Nuevo dataset →\", out_latest)\n",
    "    return df_out\n",
    "\n",
    "print(\" Funciones de conversión a audio cargadas correctamente\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
