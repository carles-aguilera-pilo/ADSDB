{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc07dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"augmented-qlora-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a578525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "minio_url = \"http://\" + os.getenv(\"S3_API_ENDPOINT\")\n",
    "\n",
    "\n",
    "minio_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url=minio_url\n",
    ")\n",
    "\n",
    "minio_bucket = \"training-preparation-zone\"\n",
    "manifest_name = \"dataset_train_augmented.json\"\n",
    "local_file = \"./dataset_train_augmented.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023c4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_manifest_from_minio(bucket_name, object_name, local_path):\n",
    "    try:\n",
    "        minio_client.download_file(bucket_name, object_name, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {object_name} from bucket {bucket_name}: {e}\")\n",
    "    return local_path\n",
    "\n",
    "downloaded_path = download_manifest_from_minio(minio_bucket, manifest_name, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388965ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 415 entries from the manifest.\n",
      "                       image                                     text  \\\n",
      "0    images/ISIC_0027249.png        texts/actinic_keratosis_0_0_0.txt   \n",
      "1    images/ISIC_0027058.png        texts/actinic_keratosis_0_0_1.txt   \n",
      "2    images/ISIC_0026152.png        texts/actinic_keratosis_0_0_2.txt   \n",
      "3    images/ISIC_0026803.png        texts/actinic_keratosis_0_0_3.txt   \n",
      "4    images/ISIC_0026077.png        texts/actinic_keratosis_0_0_4.txt   \n",
      "..                       ...                                      ...   \n",
      "410  images/ISIC_0027710.png  texts/squamous_cell_carcinoma_0_0_2.txt   \n",
      "411  images/ISIC_0031380.png  texts/squamous_cell_carcinoma_0_0_3.txt   \n",
      "412  images/ISIC_0032110.png  texts/squamous_cell_carcinoma_0_0_4.txt   \n",
      "413  images/ISIC_0032110.png  texts/squamous_cell_carcinoma_0_0_5.txt   \n",
      "414  images/ISIC_0034222.png  texts/squamous_cell_carcinoma_0_0_6.txt   \n",
      "\n",
      "        score  \n",
      "0    1.438925  \n",
      "1    1.429278  \n",
      "2    1.461677  \n",
      "3    1.439080  \n",
      "4    1.709937  \n",
      "..        ...  \n",
      "410  1.677108  \n",
      "411  1.542267  \n",
      "412  1.428125  \n",
      "413  1.544398  \n",
      "414  1.417225  \n",
      "\n",
      "[415 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_manifest(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        data = pd.read_json(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from the manifest.\")\n",
    "    return data\n",
    "\n",
    "df = load_manifest(downloaded_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67446b5",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7845cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oriol.vilella\\Documents\\ADSDB\\Part2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a39f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu126\n",
      "Is CUDA available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1277e",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "\n",
    "The inputs variable is defined as it is because the model needs all of those parameters:\n",
    "\n",
    "- Truncation=True means that if we provide more than 77 tokens (the usual maximum) it truncates the data\n",
    "\n",
    "- Padding=max_length means that we add zeros to fill the max_length. We need to provide the same length for all the data (specially in text).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be17d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, minio_client, bucket_name):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.minio_client = minio_client\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.df.iloc[idx]['image']\n",
    "        txt_key = self.df.iloc[idx]['text']\n",
    "\n",
    "        img_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=img_key)\n",
    "        img_bytes = img_response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        txt_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=txt_key)\n",
    "        description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[description], \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430fa7e",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Here we train the smaller clip model. We load it from the SkinLesionDataset class we created and the particularity is that we use AdamW. The AdamW is a widely used optimitzer for training Transformers. While the loss function tells the model where it needs to go, the optimitzer decides how fast it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c0ca4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor, BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_type=torch.float16\n",
    ")\n",
    "model = CLIPModel.from_pretrained(MODEL_ID, quantization_config=bnb_config, device_map=\"auto\").to(DEVICE)\n",
    "model.get_input_embeddings = lambda: model.text_model.embeddings.token_embedding\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "lora_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"visual_projection\", \"text_projection\"], \n",
    "    lora_dropout=0.05, \n",
    "    bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "dataset = SkinLesionDataset(df, processor, minio_client, minio_bucket)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LEARNING_RATE, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b014f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 36864\n",
      "All parameters: 89775873\n",
      "Percentage of trainable parameters: 0.04%\n"
     ]
    }
   ],
   "source": [
    "def get_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"All parameters: {all_params}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "    return trainable_params, all_params\n",
    "\n",
    "trainable, total = get_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162603b8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we train the model using the hyperparameters and all the information provided in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c737fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_vram(device):\n",
    "    if device == 'cuda':\n",
    "        return torch.cuda.max_memory_allocated(device) / (1024 ** 3)  # Convert to GB\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f9fe3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/52 [00:00<?, ?it/s]c:\\Users\\oriol.vilella\\Documents\\ADSDB\\Part2\\venv\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "c:\\Users\\oriol.vilella\\Documents\\ADSDB\\Part2\\venv\\Lib\\site-packages\\torch\\utils\\checkpoint.py:85: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n",
      "Epoch 1: 100%|██████████| 52/52 [00:26<00:00,  2.00it/s, loss=3.2] \n",
      "Epoch 2: 100%|██████████| 52/52 [00:24<00:00,  2.09it/s, loss=2.72]\n",
      "Epoch 3: 100%|██████████| 52/52 [00:24<00:00,  2.09it/s, loss=2.65]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM usage during training: 0.57 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats(DEVICE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            return_loss=True\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    loss_history.append(epoch_loss / len(dataloader))\n",
    "\n",
    "peak_mem = get_peak_vram(DEVICE)\n",
    "print(f\"Peak VRAM usage during training: {peak_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e80d41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Latency: 73.04 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_inference_latency(model, dataloader, device, num_samples=50):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_samples: break\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append((end_time - start_time) * 1000)\n",
    "            \n",
    "    avg_latency = np.mean(latencies)\n",
    "    print(f\"Average Inference Latency: {avg_latency:.2f} ms\")\n",
    "    return avg_latency\n",
    "latency = calculate_inference_latency(model, dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4934cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'Recall@1': np.float64(0.00963855421686747), 'Recall@5': np.float64(0.03855421686746988), 'Recall@10': np.float64(0.06506024096385542), 'Mean Rank': np.float64(153.8867469879518), 'Median Rank': np.float64(135.0), 'MRR': np.float64(0.035222223988940496), 'NDCG': np.float64(0.17380703444825432), 'Sensitivity': 0.19171593485607855, 'Specificity': np.float64(0.8007611832609356), 'F1 Score': 0.17398886503202257, 'BertScore Precision': 0.8452470302581787, 'BertScore Recall': 0.849989116191864, 'BERTScore F1': 0.8472861647605896, 'Trainable Parameters': 36864, 'Total Parameters': 89775873, 'Inference Latency (ms)': np.float64(73.04405689239502), 'Peak VRAM Usage (GB)': 0.5736546516418457}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score_func\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def extract_class_from_path(path):\n",
    "    return \"_\".join(path.split(\"/\")[-1].split(\"_\")[:-3])\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_comprehensive_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    all_ground_truth_texts = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        img_emb = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        txt_emb = model.get_text_features(input_ids=batch['input_ids'], \n",
    "                                        attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        all_image_embeds.append(F.normalize(img_emb, dim=-1))\n",
    "        all_text_embeds.append(F.normalize(txt_emb, dim=-1))\n",
    "\n",
    "    image_embeds = torch.cat(all_image_embeds)\n",
    "    text_embeds = torch.cat(all_text_embeds)\n",
    "\n",
    "    # Perspective 1: Text-to-Image Retrieval\n",
    "    sim_matrix = text_embeds @ image_embeds.T\n",
    "    \n",
    "    num_queries = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "\n",
    "    # Perspective 2: Safety (Clinical Classification)\n",
    "    # We pass an image, retrieve the best text and check if classes match.\n",
    "    sim_matrix_i2t = image_embeds @ text_embeds.T\n",
    "    \n",
    "    # Map all text files in the dataset to their classes\n",
    "    all_text_paths = df['text'].tolist()\n",
    "    text_classes = np.array([extract_class_from_path(p) for p in all_text_paths])\n",
    "    image_classes = np.array([extract_class_from_path(p) for p in df['text'].tolist()])\n",
    "\n",
    "    top_text_indices = torch.argmax(sim_matrix_i2t, dim=-1).cpu().numpy()\n",
    "    predicted_classes = text_classes[top_text_indices]\n",
    "\n",
    "    sensitivity = recall_score(image_classes, predicted_classes, average='macro')\n",
    "    f1 = f1_score(image_classes, predicted_classes, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(image_classes, predicted_classes)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    specificity = np.mean(tn / (tn + fp + 1e-10))\n",
    "\n",
    "    def get_text_content(path, client, bucket):\n",
    "        response = client.get_object(Bucket=bucket, Key=path)\n",
    "        return response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "    sample_indices = np.random.choice(len(df), min(50, len(df)), replace=False)\n",
    "    gt_texts = [get_text_content(df.iloc[i]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "    predicted_classes_texts = [get_text_content(df.iloc[top_text_indices[i]]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "\n",
    "    P, R, F1 = bert_score_func(predicted_classes_texts, gt_texts, lang='en', verbose=False)\n",
    "\n",
    "    metrics = {\n",
    "        # Perspective 1\n",
    "        \"Recall@1\":  np.mean(ranks <= 1),\n",
    "        \"Recall@5\":  np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"Mean Rank\": np.mean(ranks),\n",
    "        \"Median Rank\": np.median(ranks),\n",
    "        \"MRR\": np.mean(1.0 / ranks),\n",
    "        \"NDCG\": np.mean([1.0 / np.log2(r + 1) for r in ranks]),\n",
    "        # Perspective 2\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1 Score\": f1,\n",
    "        # Perspective 3\n",
    "        \"BertScore Precision\": P.mean().item(),\n",
    "        \"BertScore Recall\": R.mean().item(),\n",
    "        \"BERTScore F1\": F1.mean().item(),\n",
    "        \"Trainable Parameters\": trainable,\n",
    "        \"Total Parameters\": total,\n",
    "        \"Inference Latency (ms)\": latency,\n",
    "        \"Peak VRAM Usage (GB)\": peak_mem\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "eval_results = get_comprehensive_metrics(model, dataloader, DEVICE)\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4df0c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "final_experiment_data = {\n",
    "    \"metadata\": {\n",
    "        \"model_name\": MODEL_ID,\n",
    "        \"device_used\": DEVICE,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": EPOCHS\n",
    "        }\n",
    "    },\n",
    "    \"metrics\": eval_results,\n",
    "    \"loss_history\": loss_history\n",
    "}\n",
    "\n",
    "json_filename = f\"../results/{EXPERIMENT_NAME}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "png_filename = f\"../results/{EXPERIMENT_NAME}_loss_curve_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(final_experiment_data, f, indent=4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, marker='o', linestyle='-', color='#2ca02c', label='Training Loss')\n",
    "plt.title(\"Skin Cancer Model: Fine-Tuning Learning Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(png_filename, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "840d1450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'visualization-zone' already exists.\n",
      "Successfully uploaded qlora-model_20260102_160843.json to qlora-model/qlora-model_20260102_160843.json\n",
      "Successfully uploaded qlora-model_loss_curve_20260102_160843.png to qlora-model/qlora-model_loss_curve_20260102_160843.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "RESULTS_BUCKET = \"visualization-zone\"\n",
    "\n",
    "def setup_results_storage(client, bucket_name):\n",
    "    try:\n",
    "        client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except:\n",
    "        print(f\"Creating bucket '{bucket_name}'...\")\n",
    "        client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "def upload_experiment_assets(client, bucket_name, results_dir, json_file, png_file):\n",
    "    try:\n",
    "        client.head_bucket(Bucket=bucket_name)\n",
    "    except:\n",
    "        client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "\n",
    "    assets = [json_file, png_file]\n",
    "    \n",
    "    for asset_name in assets:\n",
    "        local_path = os.path.join(results_dir, asset_name)\n",
    "        \n",
    "        if os.path.exists(local_path):\n",
    "            # We store them in a folder named after the run_id for the viz page\n",
    "            object_key = f\"{EXPERIMENT_NAME}/{asset_name}\"\n",
    "            try:\n",
    "                client.upload_file(local_path, bucket_name, object_key)\n",
    "                print(f\"Successfully uploaded {asset_name} to {object_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {asset_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Asset not found at {local_path}\")\n",
    "\n",
    "\n",
    "json_file = os.path.basename(json_filename)\n",
    "png_file = os.path.basename(png_filename)\n",
    "\n",
    "setup_results_storage(minio_client, RESULTS_BUCKET)\n",
    "upload_experiment_assets(minio_client, RESULTS_BUCKET, \"../results\", json_file, png_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
