{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc07dc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = \"augmentation-fp16-model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a578525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "minio_url = \"http://\" + os.getenv(\"S3_API_ENDPOINT\")\n",
    "\n",
    "\n",
    "minio_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url=minio_url\n",
    ")\n",
    "\n",
    "minio_bucket = \"training-preparation-zone\"\n",
    "manifest_name = \"dataset_train_augmented.json\"\n",
    "local_file = \"./dataset_train_augmented.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "023c4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_manifest_from_minio(bucket_name, object_name, local_path):\n",
    "    try:\n",
    "        minio_client.download_file(bucket_name, object_name, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {object_name} from bucket {bucket_name}: {e}\")\n",
    "    return local_path\n",
    "\n",
    "downloaded_path = download_manifest_from_minio(minio_bucket, manifest_name, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388965ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1992 entries from the manifest.\n",
      "                                                  image  \\\n",
      "0                               images/ISIC_0025899.png   \n",
      "1       images/ISIC_0025899_brightness_1.12_flipped.png   \n",
      "2     images/ISIC_0025899_rotated_-13_contrast_1.06.png   \n",
      "3                               images/ISIC_0026803.png   \n",
      "4     images/ISIC_0026803_rotated_-2_contrast_1.19_f...   \n",
      "...                                                 ...   \n",
      "1987  images/ISIC_0029694_brightness_1.06_contrast_0...   \n",
      "1988      images/ISIC_0029694_contrast_1.15_flipped.png   \n",
      "1989                            images/ISIC_0028103.png   \n",
      "1990                  images/ISIC_0028103_augmented.png   \n",
      "1991  images/ISIC_0028103_rotated_2_contrast_0.89_fl...   \n",
      "\n",
      "                               text     score  \n",
      "0     texts/actinic_keratosis_0.txt  9934.281  \n",
      "1     texts/actinic_keratosis_0.txt  9934.281  \n",
      "2     texts/actinic_keratosis_0.txt  9934.281  \n",
      "3     texts/actinic_keratosis_1.txt  9933.505  \n",
      "4     texts/actinic_keratosis_1.txt  9933.505  \n",
      "...                             ...       ...  \n",
      "1987       texts/melanoma_0_121.txt  9973.689  \n",
      "1988       texts/melanoma_0_121.txt  9973.689  \n",
      "1989       texts/melanoma_0_122.txt  9979.729  \n",
      "1990       texts/melanoma_0_122.txt  9979.729  \n",
      "1991       texts/melanoma_0_122.txt  9979.729  \n",
      "\n",
      "[1992 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_manifest(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        data = pd.read_json(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from the manifest.\")\n",
    "    return data\n",
    "\n",
    "df = load_manifest(downloaded_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67446b5",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7845cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\oriol.vilella\\Documents\\ADSDB\\Part2\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d8a39f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.9.1+cu126\n",
      "Is CUDA available: True\n",
      "GPU Name: NVIDIA GeForce GTX 1650\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Is CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1277e",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "\n",
    "The inputs variable is defined as it is because the model needs all of those parameters:\n",
    "\n",
    "- Truncation=True means that if we provide more than 77 tokens (the usual maximum) it truncates the data\n",
    "\n",
    "- Padding=max_length means that we add zeros to fill the max_length. We need to provide the same length for all the data (specially in text).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be17d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, minio_client, bucket_name):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.minio_client = minio_client\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.df.iloc[idx]['image']\n",
    "        txt_key = self.df.iloc[idx]['text']\n",
    "\n",
    "        img_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=img_key)\n",
    "        img_bytes = img_response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        txt_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=txt_key)\n",
    "        description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[description], \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430fa7e",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Here we train the smaller clip model. We load it from the SkinLesionDataset class we created and the particularity is that we use AdamW. The AdamW is a widely used optimitzer for training Transformers. While the loss function tells the model where it needs to go, the optimitzer decides how fast it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c0ca4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(MODEL_ID, dtype=torch.bfloat16).to(DEVICE) # We change to 16-bit precision\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "dataset = SkinLesionDataset(df, processor, minio_client, minio_bucket)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3b014f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 151277313\n",
      "All parameters: 151277313\n",
      "Percentage of trainable parameters: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def get_trainable_parameters(model):\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    all_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "    print(f\"Trainable parameters: {trainable_params}\")\n",
    "    print(f\"All parameters: {all_params}\")\n",
    "    print(f\"Percentage of trainable parameters: {100 * trainable_params / all_params:.2f}%\")\n",
    "\n",
    "    return trainable_params, all_params\n",
    "\n",
    "trainable, total = get_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162603b8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we train the model using the hyperparameters and all the information provided in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c737fee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_peak_vram(device):\n",
    "    if device == 'cuda':\n",
    "        return torch.cuda.max_memory_allocated(device) / (1024 ** 3)  # Convert to GB\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f9fe3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 52/52 [00:29<00:00,  1.76it/s, loss=1.78]\n",
      "Epoch 2: 100%|██████████| 52/52 [00:28<00:00,  1.81it/s, loss=1.23]\n",
      "Epoch 3: 100%|██████████| 52/52 [00:29<00:00,  1.78it/s, loss=1.67] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak VRAM usage during training: 2.08 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "if DEVICE == 'cuda':\n",
    "    torch.cuda.reset_peak_memory_stats(DEVICE)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            return_loss=True\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    loss_history.append(epoch_loss / len(dataloader))\n",
    "\n",
    "peak_mem = get_peak_vram(DEVICE)\n",
    "print(f\"Peak VRAM usage during training: {peak_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e80d41fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Inference Latency: 19.93 ms\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "\n",
    "def calculate_inference_latency(model, dataloader, device, num_samples=50):\n",
    "    model.eval()\n",
    "    latencies = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= num_samples: break\n",
    "            \n",
    "            batch = {k: v.to(device) for k, v in batch.items() if isinstance(v, torch.Tensor)}\n",
    "            \n",
    "            start_time = time.time()\n",
    "            _ = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "            end_time = time.time()\n",
    "            \n",
    "            latencies.append((end_time - start_time) * 1000)\n",
    "            \n",
    "    avg_latency = np.mean(latencies)\n",
    "    print(f\"Average Inference Latency: {avg_latency:.2f} ms\")\n",
    "    return avg_latency\n",
    "latency = calculate_inference_latency(model, dataloader, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4934cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'Recall@1': np.float64(0.02891566265060241), 'Recall@5': np.float64(0.13493975903614458), 'Recall@10': np.float64(0.20963855421686747), 'Mean Rank': np.float64(52.11807228915663), 'Median Rank': np.float64(30.0), 'MRR': np.float64(0.09708754052159495), 'NDCG': np.float64(0.25435693884136107), 'Sensitivity': 0.21326798319078852, 'Specificity': np.float64(0.806591490946085), 'F1 Score': 0.21073547659563455, 'BertScore Precision': 0.8503291606903076, 'BertScore Recall': 0.8424186706542969, 'BERTScore F1': 0.8461728096008301, 'Trainable Parameters': 151277313, 'Total Parameters': 151277313, 'Inference Latency (ms)': np.float64(19.92967128753662), 'Peak VRAM Usage (GB)': 2.076547622680664}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score_func\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def extract_class_from_path(path):\n",
    "    return \"_\".join(path.split(\"/\")[-1].split(\"_\")[:-3])\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_comprehensive_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    all_ground_truth_texts = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        img_emb = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        txt_emb = model.get_text_features(input_ids=batch['input_ids'], \n",
    "                                        attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        all_image_embeds.append(F.normalize(img_emb, dim=-1))\n",
    "        all_text_embeds.append(F.normalize(txt_emb, dim=-1))\n",
    "\n",
    "    image_embeds = torch.cat(all_image_embeds)\n",
    "    text_embeds = torch.cat(all_text_embeds)\n",
    "\n",
    "    # Perspective 1: Text-to-Image Retrieval\n",
    "    sim_matrix = text_embeds @ image_embeds.T\n",
    "    \n",
    "    num_queries = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "\n",
    "    # Perspective 2: Safety (Clinical Classification)\n",
    "    # We pass an image, retrieve the best text and check if classes match.\n",
    "    sim_matrix_i2t = image_embeds @ text_embeds.T\n",
    "    \n",
    "    # Map all text files in the dataset to their classes\n",
    "    all_text_paths = df['text'].tolist()\n",
    "    text_classes = np.array([extract_class_from_path(p) for p in all_text_paths])\n",
    "    image_classes = np.array([extract_class_from_path(p) for p in df['text'].tolist()])\n",
    "\n",
    "    top_text_indices = torch.argmax(sim_matrix_i2t, dim=-1).cpu().numpy()\n",
    "    predicted_classes = text_classes[top_text_indices]\n",
    "\n",
    "    sensitivity = recall_score(image_classes, predicted_classes, average='macro')\n",
    "    f1 = f1_score(image_classes, predicted_classes, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(image_classes, predicted_classes)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    specificity = np.mean(tn / (tn + fp + 1e-10))\n",
    "\n",
    "    def get_text_content(path, client, bucket):\n",
    "        response = client.get_object(Bucket=bucket, Key=path)\n",
    "        return response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "    sample_indices = np.random.choice(len(df), min(50, len(df)), replace=False)\n",
    "    gt_texts = [get_text_content(df.iloc[i]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "    predicted_classes_texts = [get_text_content(df.iloc[top_text_indices[i]]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "\n",
    "    P, R, F1 = bert_score_func(predicted_classes_texts, gt_texts, lang='en', verbose=False)\n",
    "\n",
    "    metrics = {\n",
    "        # Perspective 1\n",
    "        \"Recall@1\":  np.mean(ranks <= 1),\n",
    "        \"Recall@5\":  np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"Mean Rank\": np.mean(ranks),\n",
    "        \"Median Rank\": np.median(ranks),\n",
    "        \"MRR\": np.mean(1.0 / ranks),\n",
    "        \"NDCG\": np.mean([1.0 / np.log2(r + 1) for r in ranks]),\n",
    "        # Perspective 2\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1 Score\": f1,\n",
    "        # Perspective 3\n",
    "        \"BertScore Precision\": P.mean().item(),\n",
    "        \"BertScore Recall\": R.mean().item(),\n",
    "        \"BERTScore F1\": F1.mean().item(),\n",
    "        \"Trainable Parameters\": trainable,\n",
    "        \"Total Parameters\": total,\n",
    "        \"Inference Latency (ms)\": latency,\n",
    "        \"Peak VRAM Usage (GB)\": peak_mem\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "eval_results = get_comprehensive_metrics(model, dataloader, DEVICE)\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4df0c4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "final_experiment_data = {\n",
    "    \"metadata\": {\n",
    "        \"model_name\": MODEL_ID,\n",
    "        \"device_used\": DEVICE,\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": BATCH_SIZE,\n",
    "            \"learning_rate\": LEARNING_RATE,\n",
    "            \"epochs\": EPOCHS\n",
    "        }\n",
    "    },\n",
    "    \"metrics\": eval_results,\n",
    "    \"loss_history\": loss_history\n",
    "}\n",
    "\n",
    "json_filename = f\"../results/{EXPERIMENT_NAME}_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "png_filename = f\"../results/{EXPERIMENT_NAME}_loss_curve_{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}.png\"\n",
    "\n",
    "with open(json_filename, 'w') as f:\n",
    "    json.dump(final_experiment_data, f, indent=4)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(loss_history, marker='o', linestyle='-', color='#2ca02c', label='Training Loss')\n",
    "plt.title(\"Skin Cancer Model: Fine-Tuning Learning Curve\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Average Loss\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.legend()\n",
    "plt.savefig(png_filename, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "840d1450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bucket 'visualization-zone' already exists.\n",
      "Successfully uploaded fp16-model_20260102_153151.json to fp16-model/fp16-model_20260102_153151.json\n",
      "Successfully uploaded fp16-model_loss_curve_20260102_153151.png to fp16-model/fp16-model_loss_curve_20260102_153151.png\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "import io\n",
    "\n",
    "RESULTS_BUCKET = \"visualization-zone\"\n",
    "\n",
    "def setup_results_storage(client, bucket_name):\n",
    "    try:\n",
    "        client.head_bucket(Bucket=bucket_name)\n",
    "        print(f\"Bucket '{bucket_name}' already exists.\")\n",
    "    except:\n",
    "        print(f\"Creating bucket '{bucket_name}'...\")\n",
    "        client.create_bucket(Bucket=bucket_name)\n",
    "\n",
    "def upload_experiment_assets(client, bucket_name, results_dir, json_file, png_file):\n",
    "    try:\n",
    "        client.head_bucket(Bucket=bucket_name)\n",
    "    except:\n",
    "        client.create_bucket(Bucket=bucket_name)\n",
    "        print(f\"Created bucket: {bucket_name}\")\n",
    "\n",
    "    assets = [json_file, png_file]\n",
    "    \n",
    "    for asset_name in assets:\n",
    "        local_path = os.path.join(results_dir, asset_name)\n",
    "        \n",
    "        if os.path.exists(local_path):\n",
    "            # We store them in a folder named after the run_id for the viz page\n",
    "            object_key = f\"{EXPERIMENT_NAME}/{asset_name}\"\n",
    "            try:\n",
    "                client.upload_file(local_path, bucket_name, object_key)\n",
    "                print(f\"Successfully uploaded {asset_name} to {object_key}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to upload {asset_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Warning: Asset not found at {local_path}\")\n",
    "\n",
    "\n",
    "json_file = os.path.basename(json_filename)\n",
    "png_file = os.path.basename(png_filename)\n",
    "\n",
    "setup_results_storage(minio_client, RESULTS_BUCKET)\n",
    "upload_experiment_assets(minio_client, RESULTS_BUCKET, \"../results\", json_file, png_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
