{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a578525c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "access_key_id = os.getenv(\"ACCESS_KEY_ID\")\n",
    "secret_access_key = os.getenv(\"SECRET_ACCESS_KEY\")\n",
    "minio_url = \"http://\" + os.getenv(\"S3_API_ENDPOINT\")\n",
    "\n",
    "\n",
    "minio_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=access_key_id,\n",
    "    aws_secret_access_key=secret_access_key,\n",
    "    endpoint_url=minio_url\n",
    ")\n",
    "\n",
    "minio_bucket = \"training-preparation-zone\"\n",
    "manifest_name = \"dataset_train.json\"\n",
    "local_file = \"./dataset_train.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "023c4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_manifest_from_minio(bucket_name, object_name, local_path):\n",
    "    try:\n",
    "        minio_client.download_file(bucket_name, object_name, local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {object_name} from bucket {bucket_name}: {e}\")\n",
    "    return local_path\n",
    "\n",
    "downloaded_path = download_manifest_from_minio(minio_bucket, manifest_name, local_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "388965ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 415 entries from the manifest.\n",
      "                       image                                     text  \\\n",
      "0    images/ISIC_0027249.png        texts/actinic_keratosis_0_0_0.txt   \n",
      "1    images/ISIC_0027058.png        texts/actinic_keratosis_0_0_1.txt   \n",
      "2    images/ISIC_0026152.png        texts/actinic_keratosis_0_0_2.txt   \n",
      "3    images/ISIC_0026803.png        texts/actinic_keratosis_0_0_3.txt   \n",
      "4    images/ISIC_0026077.png        texts/actinic_keratosis_0_0_4.txt   \n",
      "..                       ...                                      ...   \n",
      "410  images/ISIC_0027710.png  texts/squamous_cell_carcinoma_0_0_2.txt   \n",
      "411  images/ISIC_0031380.png  texts/squamous_cell_carcinoma_0_0_3.txt   \n",
      "412  images/ISIC_0032110.png  texts/squamous_cell_carcinoma_0_0_4.txt   \n",
      "413  images/ISIC_0032110.png  texts/squamous_cell_carcinoma_0_0_5.txt   \n",
      "414  images/ISIC_0034222.png  texts/squamous_cell_carcinoma_0_0_6.txt   \n",
      "\n",
      "        score  \n",
      "0    1.438926  \n",
      "1    1.429278  \n",
      "2    1.461677  \n",
      "3    1.439080  \n",
      "4    1.709936  \n",
      "..        ...  \n",
      "410  1.677108  \n",
      "411  1.542267  \n",
      "412  1.428125  \n",
      "413  1.544399  \n",
      "414  1.417225  \n",
      "\n",
      "[415 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def load_manifest(manifest_path):\n",
    "    with open(manifest_path, 'r') as f:\n",
    "        data = pd.read_json(f)\n",
    "    \n",
    "    print(f\"Loaded {len(data)} entries from the manifest.\")\n",
    "    return data\n",
    "\n",
    "df = load_manifest(downloaded_path)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67446b5",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b7845cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oriol/Documentos/MDS/ADSDB/Part2/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "from torch.optim import AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "EPOCHS = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c1277e",
   "metadata": {},
   "source": [
    "## Data retrieval\n",
    "\n",
    "The inputs variable is defined as it is because the model needs all of those parameters:\n",
    "\n",
    "- Truncation=True means that if we provide more than 77 tokens (the usual maximum) it truncates the data\n",
    "\n",
    "- Padding=max_length means that we add zeros to fill the max_length. We need to provide the same length for all the data (specially in text).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "be17d233",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "class SkinLesionDataset(Dataset):\n",
    "    def __init__(self, dataframe, processor, minio_client, bucket_name):\n",
    "        self.df = dataframe\n",
    "        self.processor = processor\n",
    "        self.minio_client = minio_client\n",
    "        self.bucket_name = bucket_name\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_key = self.df.iloc[idx]['image']\n",
    "        txt_key = self.df.iloc[idx]['text']\n",
    "\n",
    "        img_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=img_key)\n",
    "        img_bytes = img_response['Body'].read()\n",
    "        image = Image.open(io.BytesIO(img_bytes)).convert(\"RGB\")\n",
    "\n",
    "        txt_response = self.minio_client.get_object(Bucket=self.bucket_name, Key=txt_key)\n",
    "        description = txt_response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "        inputs = self.processor(\n",
    "            text=[description], \n",
    "            images=image, \n",
    "            return_tensors=\"pt\", \n",
    "            padding=\"max_length\", \n",
    "            truncation=True\n",
    "        )\n",
    "        \n",
    "        return {k: v.squeeze(0) for k, v in inputs.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430fa7e",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "Here we train the smaller clip model. We load it from the SkinLesionDataset class we created and the particularity is that we use AdamW. The AdamW is a widely used optimitzer for training Transformers. While the loss function tells the model where it needs to go, the optimitzer decides how fast it goes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0ca4e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model = CLIPModel.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)\n",
    "dataset = SkinLesionDataset(df, processor, minio_client, minio_bucket)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162603b8",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Here we train the model using the hyperparameters and all the information provided in the previous cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9fe3c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 52/52 [02:02<00:00,  2.36s/it, loss=1.95]\n",
      "Epoch 2: 100%|██████████| 52/52 [02:03<00:00,  2.37s/it, loss=0.629]\n",
      "Epoch 3: 100%|██████████| 52/52 [02:03<00:00,  2.37s/it, loss=0.688]\n"
     ]
    }
   ],
   "source": [
    "loss_history = []\n",
    "model.train()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    pbar = tqdm(dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "    epoch_loss = 0\n",
    "    \n",
    "    for batch in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
    "        \n",
    "        outputs = model(\n",
    "            input_ids=batch['input_ids'],\n",
    "            pixel_values=batch['pixel_values'],\n",
    "            attention_mask=batch['attention_mask'],\n",
    "            return_loss=True\n",
    "        )\n",
    "        \n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        pbar.set_postfix({\"loss\": loss.item()})\n",
    "    \n",
    "    loss_history.append(epoch_loss / len(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4934cd28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'Recall@1': np.float64(0.10843373493975904), 'Recall@5': np.float64(0.3614457831325301), 'Recall@10': np.float64(0.5542168674698795), 'Mean Rank': np.float64(16.236144578313255), 'Median Rank': np.float64(9.0), 'MRR': np.float64(0.24258346426257882), 'NDCG': np.float64(0.396844550944322)}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from bert_score import score as bert_score_func\n",
    "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
    "\n",
    "def extract_class_from_path(path):\n",
    "    return \"_\".join(path.split(\"/\")[-1].split(\"_\")[:-3])\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_comprehensive_metrics(model, dataloader, device):\n",
    "    model.eval()\n",
    "    all_image_embeds = []\n",
    "    all_text_embeds = []\n",
    "    all_ground_truth_texts = []\n",
    "\n",
    "    for batch in dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        img_emb = model.get_image_features(pixel_values=batch['pixel_values'])\n",
    "        txt_emb = model.get_text_features(input_ids=batch['input_ids'], \n",
    "                                        attention_mask=batch['attention_mask'])\n",
    "        \n",
    "        all_image_embeds.append(F.normalize(img_emb, dim=-1))\n",
    "        all_text_embeds.append(F.normalize(txt_emb, dim=-1))\n",
    "\n",
    "    image_embeds = torch.cat(all_image_embeds)\n",
    "    text_embeds = torch.cat(all_text_embeds)\n",
    "\n",
    "    # Perspective 1: Text-to-Image Retrieval\n",
    "    sim_matrix = text_embeds @ image_embeds.T\n",
    "    \n",
    "    num_queries = sim_matrix.size(0)\n",
    "    ranks = []\n",
    "    \n",
    "    for i in range(num_queries):\n",
    "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
    "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item() + 1\n",
    "        ranks.append(rank)\n",
    "    \n",
    "    ranks = np.array(ranks)\n",
    "\n",
    "    # Perspective 2: Safety (Clinical Classification)\n",
    "    # We pass an image, retrieve the best text and check if classes match.\n",
    "    sim_matrix_i2t = image_embeds @ text_embeds.T\n",
    "    \n",
    "    # Map all text files in the dataset to their classes\n",
    "    all_text_paths = df['text'].tolist()\n",
    "    text_classes = np.array([extract_class_from_path(p) for p in all_text_paths])\n",
    "    image_classes = np.array([extract_class_from_path(p) for p in df['text'].tolist()])\n",
    "\n",
    "    top_text_indices = torch.argmax(sim_matrix_i2t, dim=-1).cpu().numpy()\n",
    "    predicted_classes = text_classes[top_text_indices]\n",
    "\n",
    "    sensitivity = recall_score(image_classes, predicted_classes, average='macro')\n",
    "    f1 = f1_score(image_classes, predicted_classes, average='macro')\n",
    "\n",
    "    cm = confusion_matrix(image_classes, predicted_classes)\n",
    "    fp = cm.sum(axis=0) - np.diag(cm)\n",
    "    fn = cm.sum(axis=1) - np.diag(cm)\n",
    "    tp = np.diag(cm)\n",
    "    tn = cm.sum() - (fp + fn + tp)\n",
    "    specificity = np.mean(tn / (tn + fp + 1e-10))\n",
    "\n",
    "    def get_text_content(path, client, bucket):\n",
    "        response = client.get_object(Bucket=bucket, Key=path)\n",
    "        return response['Body'].read().decode('utf-8').strip()\n",
    "\n",
    "    sample_indices = np.random.choice(len(df), min(50, len(df)), replace=False)\n",
    "    gt_texts = [get_text_content(df.iloc[i]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "    predicted_classes_texts = [get_text_content(df.iloc[top_text_indices[i]]['text'], minio_client, minio_bucket) for i in sample_indices]\n",
    "\n",
    "    _, _, bert_f1 = bert_score_func(predicted_classes_texts, gt_texts, lang='en', verbose=False)\n",
    "\n",
    "    metrics = {\n",
    "        # Perspective 1\n",
    "        \"Recall@1\":  np.mean(ranks <= 1),\n",
    "        \"Recall@5\":  np.mean(ranks <= 5),\n",
    "        \"Recall@10\": np.mean(ranks <= 10),\n",
    "        \"Mean Rank\": np.mean(ranks),\n",
    "        \"Median Rank\": np.median(ranks),\n",
    "        \"MRR\": np.mean(1.0 / ranks),\n",
    "        \"NDCG\": np.mean([1.0 / np.log2(r + 1) for r in ranks]),\n",
    "        # Perspective 2\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"F1 Score\": f1,\n",
    "        # Perspective 3\n",
    "        \"BERTScore F1\": bert_f1.mean().item()\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "eval_results = get_comprehensive_metrics(model, dataloader, DEVICE)\n",
    "print(\"Evaluation Results:\", eval_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
